{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 解析cifar-10数据集\n",
    "运行H_ResNeXt/src/01_parse_cifar10_to_png.py，将cifar-10数据集解析为png格式   \n",
    "\n",
    "数据存放位置：   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-python.tar   \n",
    "解压得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar-10-batches-py\n",
    "\n",
    "经过01_parse_cifar10_to_png.py，得到：  \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_train   \n",
    "F:\\cv_paper\\lesson\\Data\\cifar-10\\cifar10_test  \n",
    "\n",
    "\n",
    "### 数据展示\n",
    "<img src=\"imgs/cifar10.png\" width=\"700\" heith=\"700\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     57
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "BASE_DIR = os.path.dirname(os.getcwd())\n",
    "os.environ['NLS_LANG'] = 'SIMPLIFIED CHINESE_CHINA.UTF8'\n",
    "import sys\n",
    "sys.path.append(BASE_DIR)\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "from tools.cifar10_dataset import CifarDataset\n",
    "from tools.common_tools import ModelTrainer, show_confMat, plot_line\n",
    "\n",
    "class CifarDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        assert (os.path.exists(data_dir)), \"data_dir:{} 不存在！\".format(data_dir)\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self._get_img_info()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn, label = self.img_info[index]\n",
    "        img = Image.open(fn).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        if len(self.img_info) == 0:\n",
    "            raise Exception(\"未获取任何图片路径，请检查dataset及文件路径！\")\n",
    "        return len(self.img_info)\n",
    "\n",
    "    def _get_img_info(self):\n",
    "        sub_dir_ = [name for name in os.listdir(self.data_dir) if os.path.isdir(os.path.join(self.data_dir, name))]\n",
    "        sub_dir = [os.path.join(self.data_dir, c) for c in sub_dir_]\n",
    "\n",
    "        self.img_info = []\n",
    "        for c_dir in sub_dir:\n",
    "            path_img = [(os.path.join(c_dir, i), int(os.path.basename(c_dir))) for i in os.listdir(c_dir) if\n",
    "                        i.endswith(\"png\")]\n",
    "            self.img_info.extend(path_img)\n",
    "            \n",
    "            \n",
    "def transform_invert(img_, transform_train):\n",
    "    \"\"\"\n",
    "    将data 进行反transfrom操作\n",
    "    :param img_: tensor\n",
    "    :param transform_train: torchvision.transforms\n",
    "    :return: PIL image\n",
    "    \"\"\"\n",
    "    if 'Normalize' in str(transform_train):\n",
    "        norm_transform = list(filter(lambda x: isinstance(x, transforms.Normalize), transform_train.transforms))\n",
    "        mean = torch.tensor(norm_transform[0].mean, dtype=img_.dtype, device=img_.device)\n",
    "        std = torch.tensor(norm_transform[0].std, dtype=img_.dtype, device=img_.device)\n",
    "        img_.mul_(std[:, None, None]).add_(mean[:, None, None])\n",
    "\n",
    "    img_ = img_.transpose(0, 2).transpose(0, 1)  # C*H*W --> H*W*C\n",
    "    if 'ToTensor' in str(transform_train):\n",
    "        img_ = np.array(img_) * 255\n",
    "\n",
    "    if img_.shape[2] == 3:\n",
    "        img_ = Image.fromarray(img_.astype('uint8')).convert('RGB')\n",
    "    elif img_.shape[2] == 1:\n",
    "        img_ = Image.fromarray(img_.astype('uint8').squeeze())\n",
    "    else:\n",
    "        raise Exception(\"Invalid img shape, expected 1 or 3 in axis 2, but got {}!\".format(img_.shape[2]) )\n",
    "\n",
    "    return img_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     3,
     11
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 5.8039e-01, 5.7647e-01,  ..., 5.8824e-01,\n",
      "          5.8039e-01, 5.7647e-01],\n",
      "         [0.0000e+00, 5.8431e-01, 5.8039e-01,  ..., 5.8824e-01,\n",
      "          5.8431e-01, 5.7647e-01],\n",
      "         [0.0000e+00, 5.8824e-01, 5.8431e-01,  ..., 5.8824e-01,\n",
      "          5.8431e-01, 5.8039e-01]],\n",
      "\n",
      "        [[2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         [2.9802e-08, 2.9802e-08, 2.9802e-08,  ..., 2.9802e-08,\n",
      "          2.9802e-08, 2.9802e-08],\n",
      "         ...,\n",
      "         [2.9802e-08, 6.3137e-01, 6.2745e-01,  ..., 6.3529e-01,\n",
      "          6.3137e-01, 6.2745e-01],\n",
      "         [2.9802e-08, 6.3529e-01, 6.3137e-01,  ..., 6.3922e-01,\n",
      "          6.3529e-01, 6.2745e-01],\n",
      "         [2.9802e-08, 6.3922e-01, 6.3529e-01,  ..., 6.3922e-01,\n",
      "          6.3529e-01, 6.3137e-01]],\n",
      "\n",
      "        [[0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00,\n",
      "          0.0000e+00, 0.0000e+00],\n",
      "         ...,\n",
      "         [0.0000e+00, 6.6275e-01, 6.5882e-01,  ..., 6.7059e-01,\n",
      "          6.6275e-01, 6.5882e-01],\n",
      "         [0.0000e+00, 6.6667e-01, 6.6275e-01,  ..., 6.7059e-01,\n",
      "          6.6667e-01, 6.5882e-01],\n",
      "         [0.0000e+00, 6.7059e-01, 6.6667e-01,  ..., 6.7059e-01,\n",
      "          6.6667e-01, 6.6275e-01]]]) 0\n",
      "<PIL.Image.Image image mode=RGB size=32x32 at 0x1CA0010C3C8>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1ca1d448908>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "norm_mean = [0.485, 0.456, 0.406]\n",
    "norm_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(norm_mean, norm_std),\n",
    "])\n",
    "\n",
    "# 构建MyDataset实例\n",
    "train_dir = os.path.join(BASE_DIR, \"..\", \"Data\", \"cifar-10\",  \"cifar10_train\")\n",
    "train_data = CifarDataset(data_dir=train_dir, transform=train_transform)\n",
    "\n",
    "print(train_data.__len__()) \n",
    "\n",
    "img_tensor, label = train_data.__getitem__(66)\n",
    "\n",
    "img_rgb = transform_invert(img_tensor, train_transform)\n",
    "print(img_tensor, label)\n",
    "print(img_rgb)\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 针对cifar-10的DenseNet结构\n",
    "参考：https://github.com/bamos/densenet.pytorch/blob/master/densenet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "# dense layer\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        interChannels = 4 * growthRate\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, interChannels, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(interChannels)\n",
    "        self.conv2 = nn.Conv2d(interChannels, growthRate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        out = torch.cat((x, out), 1)  # 在此体现dense connection，两个特征图拼接起来\n",
    "        return out\n",
    "\n",
    "#  BN----ReLU----Conv\n",
    "#  若不采用Bottleneck， 则采用SingleLayer， SingleLayer 与 Bottleneck 是对等的概念\n",
    "class SingleLayer(nn.Module):\n",
    "    def __init__(self, nChannels, growthRate):\n",
    "        super(SingleLayer, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, growthRate, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))\n",
    "        out = torch.cat((x, out), 1)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self, nChannels, nOutChannels):\n",
    "        super(Transition, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.conv1 = nn.Conv2d(nChannels, nOutChannels, kernel_size=1,\n",
    "                               bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(F.relu(self.bn1(x)))  # BN----ReLU----Conv\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growthRate, depth, reduction, nClasses, bottleneck):\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        nDenseBlocks = (depth-4) // 3   # 4 表示第一个卷积层 + 2个transition + 1个FC层输出分类 ；；；3表示3个block\n",
    "        if bottleneck:\n",
    "            nDenseBlocks //= 2  # 2表示一个基础操作模块中有2层\n",
    "\n",
    "        nChannels = 2*growthRate    # 第一个卷积的卷积核个数为 growthRate的两倍\n",
    "        self.conv1 = nn.Conv2d(3, nChannels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "        # 第一个 denseblock\n",
    "        self.dense1 = self._make_denseblock(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate                 # 计算经过denseblock之后有多少个通道\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))  # 计算经过Compression后，还有几个通道reduction对应论文中的θ\n",
    "        self.trans1 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        # 第二个 denseblock\n",
    "        nChannels = nOutChannels\n",
    "        self.dense2 = self._make_denseblock(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "        nOutChannels = int(math.floor(nChannels*reduction))\n",
    "        self.trans2 = Transition(nChannels, nOutChannels)\n",
    "\n",
    "        # 第三个 denseblock\n",
    "        nChannels = nOutChannels\n",
    "        self.dense3 = self._make_denseblock(nChannels, growthRate, nDenseBlocks, bottleneck)\n",
    "        nChannels += nDenseBlocks*growthRate\n",
    "\n",
    "        # 分类输出层\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels)\n",
    "        self.fc = nn.Linear(nChannels, nClasses)\n",
    "\n",
    "        # 权值初始化\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_denseblock(self, nChannels, growthRate, nDenseBlocks, bottleneck):\n",
    "        \"\"\"\n",
    "        创建denseblock\n",
    "        :param nChannels: 进入block的特征图的通道数\n",
    "        :param growthRate: int\n",
    "        :param nDenseBlocks: blocks堆叠的数量\n",
    "        :param bottleneck: boolean，是否需要bottleneck， densenet-b\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        layers = []\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            if bottleneck:\n",
    "                layers.append(Bottleneck(nChannels, growthRate))\n",
    "            else:\n",
    "                layers.append(SingleLayer(nChannels, growthRate))\n",
    "            nChannels += growthRate     # 通道数逐渐增加，从此看出block中各层的输入均包含进入block的特征图\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.dense3(out)\n",
    "        out = torch.squeeze(F.avg_pool2d(F.relu(self.bn1(out)), 8))\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseNet(\n",
      "  (conv1): Conv2d(3, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (dense1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(24, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(36, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans1): Transition(\n",
      "    (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(48, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (trans2): Transition(\n",
      "    (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1): Conv2d(120, 60, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      "  (dense3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (bn1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(60, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (bn1): BatchNorm2d(72, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(72, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (bn1): BatchNorm2d(84, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(84, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (bn1): BatchNorm2d(108, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(108, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (bn1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv1): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(48, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    )\n",
      "  )\n",
      "  (bn1): BatchNorm2d(132, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc): Linear(in_features=132, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "densenet_model = DenseNet(growthRate=12, depth=40, reduction=0.5, bottleneck=True, nClasses=10)\n",
    "print(densenet_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 模型训练器  ModelTrainer\n",
    "\n",
    "定义模型训练类，用于完成模型前向，反向传播，并记录训练loss，accuracy等指标  \n",
    "\n",
    "目的是简化主代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     4,
     43
    ]
   },
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def train(data_loader, model, loss_f, optimizer, epoch_id, device, max_epoch):\n",
    "        model.train()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))   # 混淆矩阵，用于绘图，且计算accuracy，precision，recall等指标很方便\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_f(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())                  # 记录每个iterations的loss，待会取均值就得到epochs的loss\n",
    "            acc_avg = conf_mat.trace() / conf_mat.sum()     # 利用混淆矩阵求取accuracy， 矩阵的迹 除以 总元素 \n",
    "\n",
    "            # 每10个iteration 打印一次训练信息，loss为10个iteration的平均\n",
    "            if i % 50 == 50 - 1:\n",
    "                print(\"Training: Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
    "                    epoch_id + 1, max_epoch, i + 1, len(data_loader), np.mean(loss_sigma), acc_avg))\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat\n",
    "\n",
    "    @staticmethod\n",
    "    def valid(data_loader, model, loss_f, device):\n",
    "        model.eval()\n",
    "\n",
    "        conf_mat = np.zeros((10, 10))\n",
    "        loss_sigma = []\n",
    "\n",
    "        for i, data in enumerate(data_loader):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_f(outputs, labels)\n",
    "\n",
    "            # 统计预测信息\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # 统计混淆矩阵\n",
    "            for j in range(len(labels)):\n",
    "                cate_i = labels[j].cpu().numpy()\n",
    "                pre_i = predicted[j].cpu().numpy()\n",
    "                conf_mat[cate_i, pre_i] += 1.\n",
    "\n",
    "            # 统计loss\n",
    "            loss_sigma.append(loss.item())\n",
    "\n",
    "        acc_avg = conf_mat.trace() / conf_mat.sum()\n",
    "\n",
    "        return np.mean(loss_sigma), acc_avg, conf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.混淆矩阵概念\n",
    "混淆矩阵(Confusion Matrix)常用来观察分类结果，其是一个N\\*N的方阵，N表示类别数。 \n",
    "\n",
    "混淆矩阵的行表示真实类别，列表示预测类别。例如，猫狗的二分类问题，有猫的图像10张，狗的图像30张，模型对这40张图片进行预测，得到的混淆矩阵为\n",
    "\n",
    "| 类别|  阿猫   | 阿狗  |\n",
    "|----|  ----  | ----  |\n",
    "|阿猫 | 7  | 3 |\n",
    "|阿狗| 10  | 20 |\n",
    "\n",
    "\n",
    "从第一行中可知道，10张猫的图像中，7张预测为猫，3张预测为狗，猫的召回率(Recall)为7/10 = 70%，   \n",
    "从第二行中可知道，30张狗的图像中，8张预测为猫，22张预测为狗，狗的召回率为20/30 = 66.7%，  \n",
    "从第一列中可知道，预测为猫的17张图像中，有7张是真正的猫，猫的精确度(Precision)为7 / 17 = 41.17%   \n",
    "从第二列中可知道，预测为狗的23张图像中，有20张是真正的狗，狗的精确度(Precision)为20 / 23 = 86.96%  \n",
    "\n",
    "模型的准确率(Accuracy)为  (7+20) / 40 = 67.5%   \n",
    "\n",
    "可以发现通过混淆矩阵可以清晰的看出网络模型的分类情况，若再结合上颜色可视化，可方便的看出模型的分类偏好。  \n",
    "\n",
    "\n",
    "<img src=\"imgs/Confusion_Matrixtrain.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.4_gpu",
   "language": "python",
   "name": "pytorch_1.4_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
